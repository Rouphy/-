# 面试题

### 测试流程相关

#### 功能测试流程

1. 在项目开始阶段由产品经理编写出产品需求文档和原型图，召开需求评审会，澄清需求并讨论需求的合理性，待需求定稿后将需求文档和原型图给测试组；测试组会根据需求文档和原型图拆分功能，提取测试点，绘制思维导图；
2. 召开测试需求评审会，编写测试计划和测试方案；测试人员编写测试用例，进行用例评审；
3. 搭建测试环境，进行冒烟测试，通过后进行系统集成测试，执行测试用例；
4. 提交bug并跟踪bug，待bug修复后进行回归测试
5. 编写测试报告，进行软件质量评估，将项目发布至预发布环境进行灰度测试，测试主功能流程；
6. 发布至生产环境（线上环境），项目总结并准备下一个版本的测试

#### 接口测试流程

1. 由开发提供接口测试文档，测试人员根据接口测试文档提取测试点，编写接口测试用例，进行用例评审；
2. 搭建接口测试环境，构建接口请求，执行接口测试用例，引入参数化和断言，连接数据库进行垃圾数据回收，异常处理等
3. 编写接口测试报告，评估接口测试结果，准备功能测试

#### 接口自动化测试流程（JMeter+Ant+Jenkins）

1. 根据开发提供的接口文档熟悉每个接口要对应实现的功能，服务器的地址、端口、接口地址、请求方式、请求参数和约束条件，熟悉响应数据，响应字段、正确和错误的响应码、对应的响应信息；
2. 编写接口测试用例，设计正常异常的测试用例，进行接口用例评审，构建接口请求，执行测试用例；
3. 建立线程组，再添加http请求，填写好请求地址，端口和请求参数，设置参数化，添加断言等，如果有依赖数据，进行提取后进行引用，添加查看结果树运行；
4. 运行完后检查接口是否通过，如果不通过，先定位原因，如果是请求参数有问题，修改后再进行测试，如果接口本身存在bug，则把服务器上的日志取下来，提单给开发修改，进行回归测试；等到接口bug修复后，就可以放到搭建好的Ant+JMeter+Jenkins框架上做持续集成测试
5. 最后自动生成接口测试报告，评估接口测试结果，准备功能测试

#### python接口自动化测试流程（Python+requests+pytest+allure+Jenkins）

1. 在PyCharm里新建一个项目
2. 采用po模式分层设计分为公共层，测试数据层和业务层：
   - `public` 层：存放常用的请求方法和自定义工具类，DB 数据库类等；
   - `data` 层：基于代码和数据分离的原则，参数化的数据文件存放在data层；
   - `conf` 配置层：存放一些配置文件，比如`log.conf`配置日志信息，日志输出格式等；
   - `logs` 日志层：日志文件的存放目录；
   - `testCase` 测试用例层：执行接口测试用例的方法，还可以引入参数化、断言、连接数据库进行垃圾数据回收、异常处理、日志处理,生成html测试报告等；
   - `temp`、`reports` 测试报告层：html 测试报告存放目录；
3. 脚本调试好，没有问题了，使用Jenkins进行持续集成，设置定时任务，定时跑测试脚本，生成html测试报告

#### APP测试流程（Android环境搭建流程）

1. 产品经理提供需求文档明确App测试需求，详细了解APP要实现功能的业务流程、版本、平台、设备等信息；
2. 准备硬件环境，根据测试需求，准备不同型号、不同分辨率、不同操作系统版本的手机或者平板等测试设备，可以是真机或者模拟器
3. 配置软件环境，安装被测APP，相关依赖软件和测试工具，对于安卓应用的测试，需要安装Android SDK配置环境变量，然后通过USB有线或者无线方式将手机与电脑相连
4. 编写测试用例，进行用例评审，将被测试的APP安装包APK上传到真机中进行安装，观察安装是否成功，是否有报错提示，记录安装耗时等信息
5. 执行测试，根据测试用例进行系统测试，包括功能测试、性能测试、兼容性测试、易用UI界面测试、交叉事件、安装卸载升级、专项测试、弱网测试（2G、3G、4G、5G、Wi-Fi）下APP的运行情况等；安装过程中，实时记录APP的运行情况，包括是否出现闪退、崩溃、无响应等问题；同时可借助ADB工具记录CPU、内存使用情况、电池温度、流量、电量消耗等性能数据
6. 根据收集到的数据和问题记录，分析问题产生的原因，判断是兼容性问题还是其他问题，对于发现的兼容性问题，明确是机型、系统版本分辨率还是其他因素相关，并整理成缺陷报告，反馈给开发人员进行修复，缺陷修改完毕后，进行验收测试
7. 验收测试通过后编写APP测试报告，记录测试过程和结果，对软件质量进行评估，为下一轮测试做准备

#### 性能测试流程

1. 性能需求分析，挑选用户使用最频繁的功能做性能测试，比如登录、打开系统首页、搜索、提交订单；确认性能指标，比如事务通过率为100%，90%的事务响应时间不超过3秒，CPU和内存的使用率为70%以下
2. 搭建性能测试环境，准备好性能测试数据，包括基础数据和使用数据两部分；例如我们要测试系统半年内是否能稳定提供服务，就要在估算一下半年内，系统会有多少数据，然后在数据库上导入对应量的数据，这叫基础数据；使用数据则是我们测试过程中需要用到的参数化的数据
3. 使用BadBoy工具，录制脚本并优化，添加参数化，断言，建立接口关联数据
4. 设计性能测试场景，我们这个项目做了单用户单功能循环200次的基准测试.，1200，1600，2000个用户，执行15分钟的压力测试，看系统最佳的并发用户数，还有看系统有没有性能瓶颈；
5. 我们搭建了分布式压力测试环境进行测试，每台压力机并发800个用户并监控linux服务器的CPU，内存，IO
6. 分析性能测试结果，如果有问题（性能瓶颈），收集相关的日志提单给开发修改，开发修改好后，回归性能测试
8. 编写性能测试报告，提供测试和更改措施

### 以往经验

#### 简述一下之前的Spring Boot项目

我做的项目是一个基于Spring Boot + Vue的公考学习平台，主要面向准备公务员考试的用户。平台分为客户端和管理端，客户端提供注册登录、课程学习、题库练习、模拟考试、错题本等功能，让考生可以系统化学习和刷题；管理端提供课程管理、题库管理、考试管理、用户管理等功能，方便教师和管理员维护平台内容。技术上，后端用Spring Boot + MyBatis-Plus实现业务逻辑，前端用Vue3开发页面，数据库用MySQL，并结合Redis缓存提升性能，使用JWT做权限控制。我之前的项目大致就是这样。

#### 简述服务拆分、异步通信、负载均衡

##### 服务拆分（Service Split/微服务）

就是把一个大系统按功能拆成多个小服务，比如用户服务、课程服务、考试服务等，每个服务独立开发、部署和维护。这样能降低模块之间的耦合，提高开发效率，也方便后期扩展

##### 异步通信（Asynchronous Communication）

就是不同服务之间不需要立刻等待对方响应，可以先把请求或者数据放到消息队列里，服务再按需要去处理。这样可以提高系统的吞吐量，减少高并发时的阻塞

##### 负载均衡（Load Balancing）

就是把大量用户请求合理分配到多台服务器上运行，防止单台机器压力过大。常见方法有Nginx、LVS、F5等。这样既能提升性能，又能在一台服务器宕机时保证系统可用

### 自动化测试脚本如何维护

- 保持脚本的可读性
- 定期进行代码审查
- 使用版本控制系统
- 自动化测试环境的稳定性
- 脚本的模块化设计

### UI自动化测试遇到元素定位不到怎么办

#### case 1：页面元素没有及时加载

加等待时间，等待时间太短也有可能出现这个问题；用强制等待或者隐式等待，提升脚本效率就用显示等待

#### case 2：页面元素不可见或不可点击

页面渲染完成的情况下，有可能元素被遮挡，直接用JavaScript定位和实现动作执行

#### case 3：页面元素是动态的

根据其他的静态属性去定位，或者根据父级，子级或同级元素定位

### 自动化测试中如何处理验证码

在Postman中测试的话可以用mock环境模拟验证码返回正常和异常的场景；也可以先对验证码进行屏蔽或者设置一个万能的验证码

### 你们有几台服务器

一般测试环境：

- 一台WEB服务器（Nginx、tomcat、weblogic、Jboss等）
- 一台DB（MySQL、Oracle、SQL server等）服务器

一般生产环境：

- 至少两台WEB服务器
- 两台DB服务器

为了防止线上服务器出现问题，或者将压力分散到其他服务器

### 开发改了代码，但是测试环境还是有问题，分析一下原因

1. 可能是开发改了代码但没有更新到最新分支，测试环境更新后，导致开发环境和测试环境的代码不一致
2. 测试环境不稳定、服务器问题等
3. 可能没有清理缓存
4. 可能没有对页面进行刷新

### 安装手册和用户操作手册是谁写的

一般是产品经理，或者我们测试进行编写
